{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3c4e00-c7b7-4360-8d08-ad1410eef31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from skimage import io, img_as_float32\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Dataset Class\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.data.iloc[idx, 1]\n",
    "        image = img_as_float32(io.imread(img_name, as_gray=True))\n",
    "        target_name = self.data.iloc[idx, 2]\n",
    "        target = img_as_float32(io.imread(target_name))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            target = self.transform(target)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "\n",
    "# Simple CNN Model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 1, kernel_size=7, stride=1, padding=3, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# UNet Model\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        self.enc_conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.enc_conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.bottleneck = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.up_conv = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec_conv = nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.final_conv = nn.Conv2d(64, 1, kernel_size=1, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = torch.relu(self.enc_conv1(x))\n",
    "        x2 = torch.relu(self.enc_conv2(self.pool(x1)))\n",
    "        bottleneck = torch.relu(self.bottleneck(self.pool(x2)))\n",
    "        up = torch.relu(self.up_conv(bottleneck))\n",
    "        dec = torch.relu(self.dec_conv(up))\n",
    "        output = self.final_conv(dec)\n",
    "        return output\n",
    "\n",
    "\n",
    "# Utility function to save outputs\n",
    "def save_images(images, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for idx, img in enumerate(images):\n",
    "        img = img.squeeze().cpu().numpy()\n",
    "        Image.fromarray((img * 255).astype('uint8')).save(os.path.join(output_dir, f\"output_{idx}.png\"))\n",
    "\n",
    "\n",
    "# Main function\n",
    "def main(args):\n",
    "    # Prepare dataset and dataloader\n",
    "    dataset_dir = args.dataset\n",
    "\n",
    "\n",
    "    # Get the list of common files\n",
    "    files = os.listdir(dataset_dir)\n",
    "    file_nums = list(set([file.split(\"_\")[0] for file in files if file.endswith(\".png\")]))\n",
    "    file_nums.sort()\n",
    "    # Define the CSV file path\n",
    "    csv_file_path = 'file_locations.csv'\n",
    "    input_str = '_input.png'\n",
    "    target_str = '_target.png'\n",
    "    # Create and write to the CSV file\n",
    "    with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "        csvwriter = csv.writer(csvfile)\n",
    "        # Write the header\n",
    "        csvwriter.writerow(['File', 'input_path', 'target_path'])\n",
    "        # Write the file locations\n",
    "        for file in file_nums:\n",
    "            input_path = os.path.join(dataset_dir, file + input_str)\n",
    "            target_path = os.path.join(dataset_dir, file + target_str)\n",
    "            csvwriter.writerow([file, input_path, target_path])\n",
    "\n",
    "    print(f'CSV file created at {csv_file_path}')\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((256, 256))])\n",
    "    dataset = ImageDataset(csv_file=csv_file_path, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    # Model selection for three channels\n",
    "    def create_model(arch):\n",
    "        if arch == \"cnn\":\n",
    "            return SimpleCNN()\n",
    "        elif arch == \"unet\":\n",
    "            return UNet()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model architecture. Choose 'cnn' or 'unet'.\")\n",
    "\n",
    "    models = {\n",
    "        \"channel_1\": create_model(args.model).to(args.device),\n",
    "        \"channel_2\": create_model(args.model).to(args.device),\n",
    "        \"channel_3\": create_model(args.model).to(args.device),\n",
    "    }\n",
    "\n",
    "    if args.mode == \"train\":\n",
    "        # Training mode\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizers = {\n",
    "            name: optim.Adam(model.parameters(), lr=0.001) for name, model in models.items()\n",
    "        }\n",
    "\n",
    "        num_epochs = 10\n",
    "        for epoch in range(num_epochs):\n",
    "            for channel, model in models.items():\n",
    "                model.train()\n",
    "                running_loss = 0.0\n",
    "                optimizer = optimizers[channel]\n",
    "                for images, targets in tqdm(dataloader, desc=f\"{channel} - Epoch {epoch+1}/{num_epochs}\"):\n",
    "                    images, targets = images.to(args.device), targets.to(args.device)\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    running_loss += loss.item()\n",
    "                print(f\"{channel} - Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(dataloader)}\")\n",
    "\n",
    "                # Save model for each channel\n",
    "                torch.save(model.state_dict(), f\"{args.model}_{channel}_model.pth\")\n",
    "                print(f\"{channel} model saved to {args.model}_{channel}_model.pth\")\n",
    "\n",
    "    elif args.mode == \"test\":\n",
    "        # Testing mode\n",
    "        outputs = {channel: [] for channel in models}\n",
    "        for channel, model in models.items():\n",
    "            model_path = f\"{args.model}_{channel}_model.pth\"\n",
    "            if not os.path.exists(model_path):\n",
    "                raise FileNotFoundError(f\"No saved model found for {channel}. Please train the model first.\")\n",
    "            model.load_state_dict(torch.load(model_path))\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for images, _ in tqdm(dataloader, desc=f\"{channel} - Running Inference\"):\n",
    "                    images = images.to(args.device)\n",
    "                    output = model(images)\n",
    "                    outputs[channel].append(output)\n",
    "\n",
    "        # Save outputs for each channel\n",
    "        for channel, channel_outputs in outputs.items():\n",
    "            save_images(channel_outputs, os.path.join(args.output_dir, channel))\n",
    "            print(f\"{channel} inference outputs saved to {os.path.join(args.output_dir, channel)}\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode. Choose 'train' or 'test'.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Train or test with multiple models for separate channels\")\n",
    "    parser.add_argument(\"--dataset\", type=str, required=True, help=\"Path to dataset CSV file\")\n",
    "    parser.add_argument(\"--model\", type=str, choices=[\"cnn\", \"unet\"], required=True, help=\"Model type: 'cnn' or 'unet'\")\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"inference\", help=\"Directory to save inference outputs\")\n",
    "    parser.add_argument(\"--mode\", type=str, choices=[\"train\", \"test\"], required=True, help=\"Mode: 'train' or 'test'\")\n",
    "    parser.add_argument(\"--device\", type=str, default=\"cuda\" if torch.cuda.is_available() else \"cpu\", help=\"Device to use\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    main(args)\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "python your_script.py \\\n",
    "    --dataset path/to/dataset.csv \\\n",
    "    --model cnn \\\n",
    "    --mode train \\\n",
    "    --device cuda\n",
    "\n",
    "\n",
    "python your_script.py \\\n",
    "    --dataset path/to/dataset.csv \\\n",
    "    --model cnn \\\n",
    "    --mode test \\\n",
    "    --output_dir path/to/output \\\n",
    "    --device cuda\n",
    "\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
